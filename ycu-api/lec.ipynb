{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecを用いた類語の取得\n",
    "<!--\n",
    "[学習済みのword2vecのモデル](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "[学習済みのword2vecのモデルのロード](https://blog.amedama.jp/entry/gensim-fasttext-pre-trained-word-vectors)\n",
    "-->\n",
    "[データ](https://fasttext.cc/docs/en/crawl-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz --output cc.ja.300.vec.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力用のディレクトリを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "out_dir='./out'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 読み込みに時間がかかる(10分程度)\n",
    "\n",
    "- pickle でダンプしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import gensim\n",
    "binary=True\n",
    "data_path='wiki.ja.vec'; binary=False\n",
    "data_path='cc.ja.300.vec.gz'; binary=False\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(data_path, binary=binary,\n",
    "                                                        unicode_errors='ignore')\n",
    "\n",
    "import pickle as pkl\n",
    "out_path=os.path.join(out_dir,data_path+'.pkl')\n",
    "with open(out_path,'wb') as fw:\n",
    "    pkl.dump(model,fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前保存のデータを読み込み(直接読むより早い)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle as pkl\n",
    "data_path='wiki.ja.vecl'\n",
    "data_path='cc.ja.300.vec.gz'\n",
    "in_path=os.path.join(out_dir,data_path+'.pkl')\n",
    "with open(in_path,'rb') as fr:\n",
    "    model0=pkl.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model0.index_to_key)[:10])\n",
    "print(len(model0.index_to_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model0.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0['猫'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# model0 = gensim.models.Word2Vec.load('./ja/ja.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_synonyms_w2v(text,model):\n",
    "    results = []\n",
    "    #for word, sim in model.wv.most_similar(text, topn=10):\n",
    "    for word, sim in model.most_similar(text, topn=10):\n",
    "        results.append({'term': word, 'similarity': sim})\n",
    "    return results\n",
    "\n",
    "\n",
    "def calc_similarity_w2v(text1, text2,model):\n",
    "    sim = model.wv.similarity(text1, text2)\n",
    "    return sim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_words=['フランス','東京','ニューヨーク',\n",
    "             #'自由の女神',\n",
    "             'ウルトラマン',\n",
    "             '中学校',\n",
    "             '政府',\n",
    "             #'6月',\n",
    "             #'テレビドラマ',\n",
    "             'ピカソ',\n",
    "             '夏',\n",
    "             'WTO',\n",
    "             'SARS',\n",
    "            ]\n",
    "\n",
    "for probe_word in probe_words:\n",
    "    word_similar=get_synonyms_w2v(probe_word,model0)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{probe_word} 〜　\\n{words}\\n')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アナロジー　（意味の加減算)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(X_Y, x,model):\n",
    "    X, Y = X_Y\n",
    "    results = []\n",
    "    #for word, sim in model.wv.most_similar(positive=[Y, x], negative=[X], topn=10):\n",
    "    for word, sim in model.most_similar(positive=[Y, x], negative=[X], topn=10):\n",
    "        results.append({'term': word, 'similarity': sim})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Y_x_list=[( 'フランス','パリ','ドイツ'),  #首都\n",
    "            ('イギリス','ロンドン','日本'),  #首都\n",
    "            ('日本','東京','ドイツ'),  #首都\n",
    "            ('少年','少女','夫'),  #性別\n",
    "            ('日本','東京', '韓国' ),  # NG 韓国がボキャブラリー辞書にない\n",
    "            ('日本','東京', '中国'),   # NG　中国がボキャブラリー辞書にない\n",
    "            ('パリ', 'エッフェル塔','東京'), # 観光資源\n",
    "           ]\n",
    "\n",
    "for X,Y, x in X_Y_x_list:\n",
    "    word_similar=analogy((X,Y),x,model0)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{Y} - {X} + {x} 〜　\\n{words}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日本語Wikipedia 全ページの文章でWord2Vecを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "#import lec05_parser as parser\n",
    "import MeCab\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クレンジング：正規化と空白の圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_table=str.maketrans(dict(zip('()!', '（）！')))  # 半角 -> 全角  \n",
    "#\n",
    "#  クレンジング：　正規化と，　空白の圧縮\n",
    "#\n",
    "def cleanse(text,debug=False):\n",
    "    if debug:\n",
    "        return text\n",
    "    text=unicodedata.normalize('NFKC', text).translate(translation_table)  #  テキストの正規化\n",
    "    text=re.sub(r'\\s+', ' ',text)  # 空白の正規化 半角の空白1つに。\n",
    "    #text=cleanse_post(text)\n",
    "    return text\n",
    "\n",
    "def insert_nl_at_eos_char(text):  # insert newline's\n",
    "    text=re.sub('([。])', '\\\\1\\n',text)\n",
    "    return text\n",
    "\n",
    "def cleanse_ext(text):  #  for word2vec special\n",
    "    text=re.sub('Section::::','',text)\n",
    "    text=re.sub('[！「」『』（），、]','',text)\n",
    "    return text\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文単位で分かち書きされたファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパス作り\n",
    "WIKIDUMP_DIR='./wikipedia_text'\n",
    "eos_mark='<EOS>'\n",
    "\n",
    "def get_each_contetns_by_text(wikidir):\n",
    "    for file in tqdm(glob(os.path.join(wikidir,'*/wiki_*'))):\n",
    "        #print(file)\n",
    "        with open(file,'r') as f:\n",
    "            read_text=f.read()\n",
    "            soup=BeautifulSoup(read_text)\n",
    "            for doc in soup.find_all(['doc']):\n",
    "                text=''\n",
    "                input_text=doc.text\n",
    "                input_text=cleanse(input_text)\n",
    "                input_text=insert_nl_at_eos_char(input_text)\n",
    "                for line in input_text.split('\\n'):\n",
    "                    line=line.strip()\n",
    "                    if len(line)>0:\n",
    "                        #print(f'**{line}**')\n",
    "                        text += cleanse_ext(line)\n",
    "                        if line[-1] in ['。','！']:\n",
    "                            text=text[:-1]+eos_mark  # remove ['。','！'] but insert eos\n",
    "                        else:\n",
    "                            text+=eos_mark\n",
    "                            #\n",
    "                            pass\n",
    "                        text += '\\n'\n",
    "                title=doc['title']\n",
    "                yield cleanse(text), cleanse(title)\n",
    "                #yield text, title\n",
    "    \n",
    "def convert_and_concat_wakati(out_dir='.'):\n",
    "    with open(os.path.join(out_dir,'wiki_wakati_nl_Q.txt'),'w') as fw,\\\n",
    "        open(os.path.join(out_dir,'wiki_title_Q.txt'),'w') as f_title:\n",
    "        count=0    \n",
    "        #mecab=parser.MeCab('-Owakati')\n",
    "        mecab= MeCab.Tagger('-Owakati')\n",
    "        for text, title in  get_each_contetns_by_text(WIKIDUMP_DIR):\n",
    "            print('　'*60,end='\\r')\n",
    "            print(f'{count}:{title[:45]}',end='\\r')\n",
    "            f_title.write(f'{count}:{title}\\n')\n",
    "            #print(mecab.parse(text))\n",
    "            wakati_words=mecab.parse(text)\n",
    "            #wakati_words=wakati.parse(text)   # こっちは エッフェル塔 が分解される\n",
    "            wakati_words=repair_eos(wakati_words)\n",
    "            fw.write(wakati_words)\n",
    "            #print(text)\n",
    "            if count>1:\n",
    "                #break\n",
    "                pass\n",
    "            count += 1\n",
    "            #if count>5: break  # for debug\n",
    "        print('done')\n",
    "        del mecab\n",
    "        \n",
    "def repair_eos(text):\n",
    "    text=re.sub('< EOS > ','\\n', text)\n",
    "    return text\n",
    "    \n",
    "# def repair_eos(text):\n",
    "#     text = '<SOS> '+text\n",
    "#     text=re.sub('< EOS >','<EOS>\\n<SOS>', text)\n",
    "#     text=re.sub('<SOS>$','',text)\n",
    "#     #print(f'* {text} *')\n",
    "#     return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分かち書き処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "convert_and_concat_wakati(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  実行の記録\n",
    "2020/11/11 <br>\n",
    "```\n",
    "見出し数：1,179,168\n",
    "分かち書き処理：\n",
    "CPU times: user 20min 54s, sys: 3min 18s, total: 24min 12s\n",
    "Wall time: 1h 18min 2s\n",
    "```\n",
    "\n",
    "2021/11/18<br>\n",
    "```\n",
    "done101:福生市立福生第五小学校　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\n",
    "CPU times: user 21min 48s, sys: 1min 49s, total: 23min 38s\n",
    "Wall time: 24min 26s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensimのword2vec で\n",
    "# 日本語wikipedia全コンテンツを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "vector_size=100\n",
    "vector_size=300\n",
    "%time data = word2vec.Text8Corpus(os.path.join(out_dir,'wiki_wakati_nl_Q.txt'))\n",
    "%time model =  word2vec.Word2Vec(data, vector_size=vector_size)  # 100次元で学習\n",
    "#\n",
    "out_fname=f\"wiki_{vector_size}.model\"\n",
    "out_path=os.path.join(out_dir,out_fname)\n",
    "print(f'saving {out_path} ... ')\n",
    "%time model1.save(out_path)  \n",
    "print(\"ok\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_fname=f\"wiki_{vector_size}.model\"\n",
    "out_path=os.path.join(out_dir,out_fname)\n",
    "print(f'saving {out_path} ... ')\n",
    "%time model1.save(out_path)  \n",
    "print(\"ok\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  実行の記録\n",
    "2020/11/09 <br>\n",
    "```\n",
    "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
    "Wall time: 9.06 µs\n",
    "CPU times: user 1h 34min 30s, sys: 1min 7s, total: 1h 35min 38s\n",
    "Wall time: 36min 14s\n",
    "CPU times: user 2.07 s, sys: 1.13 s, total: 3.2 s\n",
    "Wall time: 3.9 s\n",
    "ok\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "ファイルサイズ：\n",
    "wiki_wakati_nl.txt                 3,147,077,302 バイト（ディスク上の3.15 GB）\n",
    "wiki.model                         67,779,585 バイト（ディスク上の70.8 MB）\n",
    "wiki.model.trainables.syn1neg.npy  373,465,328 バイト（ディスク上の376.9 MB）\n",
    "wiki.model.wv.vectors.npy          373,465,328 バイト（ディスク上の375.7 MB）\n",
    "```\n",
    "\n",
    "2021.11.18<br>\n",
    "```\n",
    "vector_size=100\n",
    "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
    "Wall time: 11.9 µs\n",
    "CPU times: user 1h 17min 40s, sys: 41.5 s, total: 1h 18min 21s\n",
    "Wall time: 27min 34s\n",
    "CPU times: user 210 ms, sys: 920 ms, total: 1.13 s\n",
    "Wall time: 1.19 s\n",
    "ok\n",
    "```\n",
    "\n",
    "生成ファイル<br>\n",
    "```\n",
    "15787032  11 18 14:19 wiki.model\n",
    "177484928 11 18 14:19 wiki.model.syn1neg.npy\n",
    "177484928 11 18 14:19 wiki.model.wv.vectors.npy\n",
    "```\n",
    "\n",
    "```\n",
    "vector_size=300\n",
    "CPU times: user 8 µs, sys: 20 µs, total: 28 µs\n",
    "Wall time: 31 µs\n",
    "CPU times: user 1h 48minCPU times: user 8 µs, sys: 20 µs, total: 28 µs\n",
    "Wall time: 31 µs\n",
    "CPU times: user 1h 48min\n",
    "saving wiki_300.model ... \n",
    "CPU times: user 249 ms, sys: 2.2 s, total: 2.45 s\n",
    "Wall time: 2.67 s\n",
    "oksaving wiki_300.model ... \n",
    "CPU times: user 249 ms, sys: 2.2 s, total: 2.45 s\n",
    "Wall time: 2.67 s\n",
    "ok\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習した Word2Vec の利用 (モデルロード）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "vector_size=100\n",
    "vector_size=300\n",
    "\n",
    "in_fname=f\"wiki_{vector_size}.model\"\n",
    "in_path=os.path.join(out_dir,in_fname)\n",
    "print(f'loading {in_path} ... ')\n",
    "model1 = gensim.models.Word2Vec.load(in_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同義語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_words=['フランス','東京','ニューヨーク',\n",
    "             #'自由の女神','ウルトラマン',\n",
    "             #'中学校',\n",
    "             '政府',\n",
    "             #'6月',\n",
    "             #'テレビドラマ',\n",
    "             'ピカソ','夏','WTO','SARS','コロナ']\n",
    "for probe_word in probe_words:\n",
    "    word_similar=get_synonyms_w2v(probe_word,model1.wv)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{probe_word} 〜　\\n{words}\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# アナロジー　（意味の加減算)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size=100\n",
    "in_fname=f\"wiki_{vector_size}.model\"\n",
    "in_path=os.path.join(out_dir,in_fname)\n",
    "print(f'loading {in_path} ... ')\n",
    "model100 = gensim.models.Word2Vec.load(in_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Y_x_list=[( 'フランス','パリ','ドイツ'),  #首都\n",
    "            ('イギリス','ロンドン','日本'),  #首都\n",
    "            ('日本','東京','ドイツ'),  #首都\n",
    "            ('少年','少女','夫'),  #性別\n",
    "            ('日本','東京', '韓国' ),  # NG 韓国がボキャブラリー辞書にない\n",
    "            ('日本','東京', '中国'),   # NG　中国がボキャブラリー辞書にない\n",
    "            #('パリ', 'エッフェル塔','東京'), # 観光資源\n",
    "           ]\n",
    "\n",
    "for X,Y, x in X_Y_x_list:\n",
    "    print('model_300')\n",
    "    print('*'*30)\n",
    "    word_similar=analogy((X,Y),x,model1.wv)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{Y} - {X} + {x} 〜　\\n{words}\\n')\n",
    "    #----\n",
    "    print('model_100')\n",
    "    word_similar=analogy((X,Y),x,model100.wv)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{Y} - {X} + {x} 〜　\\n{words}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ <br>\n",
    "\n",
    "# ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ <br>\n",
    "\n",
    "\n",
    "# ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ <br>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
