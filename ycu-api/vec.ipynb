{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecを用いた類語の取得\n",
    "<!--\n",
    "[学習済みのword2vecのモデル](https://github.com/Kyubyong/wordvectors)\n",
    "\n",
    "[学習済みのword2vecのモデルのロード](https://blog.amedama.jp/entry/gensim-fasttext-pre-trained-word-vectors)\n",
    "-->\n",
    "[データ](https://fasttext.cc/docs/en/crawl-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz --output cc.ja.300.vec.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力用のディレクトリを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/workspaces/ycu-gradiation/DataSet/wiki-news-300d-1M.vec'\n",
    "type(load_vectors(data_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 読み込みに時間がかかる(10分程度)\n",
    "\n",
    "- pickle でダンプしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspaces/ycu-gradiation/wiki-news-300d-1M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         )\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspaces/ycu-gradiation/wiki-news-300d-1M.vec'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "binary=True\n",
    "# data_path='/workspaces/ycu-gradiation/DataSet/cc.en.300.vec.gz'; binary=False \n",
    "# data_path='/workspaces/ycu-gradiation/DataSet/wiki.en.vec' ; binary=False  \n",
    "# data_path='/workspaces/ycu-gradiation/wiki.en.align.vec' ; binary=False  \n",
    "data_path='/workspaces/ycu-gradiation/wiki-news-300d-1M.vec' ; binary=False  \n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(data_path, binary=binary,\n",
    "                                                        unicode_errors='ignore')\n",
    "\n",
    "import pickle as pkl\n",
    "out_path=os.path.join(data_path+'.pkl')\n",
    "with open(out_path,'wb') as fw:\n",
    "    pkl.dump(model,fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前保存のデータを読み込み(直接読むより早い)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 3.02 s, total: 4.8 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle as pkl\n",
    "# data_path='wiki.en.vec'\n",
    "# data_path='cc.en.300.vec.gz'\n",
    "# data_path='wiki.en.align.vec'\n",
    "data_path='wiki-news-300d-1M.vec'\n",
    "# data_path='wiki.en.vec'\n",
    "out_dir = \"/workspaces/test/ycu-api/DataSet\"\n",
    "in_path=os.path.join(out_dir,data_path+'.pkl')\n",
    "with open(in_path,'rb') as fr:\n",
    "    model0=pkl.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "999994\n"
     ]
    }
   ],
   "source": [
    "print(list(model0.index_to_key)[:50])\n",
    "print(len(model0.index_to_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model0.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0['cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14056/2471884422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0men_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/workspaces/test/ycu-api/DataSet/english_words_list.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lemma\\n（見出し語）'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Rank\\n(順位)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Frequency\\n(頻度)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Commentary\\n(解説)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frequency\\n(頻度)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# en_word_list.dropna().info()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_word_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "en_word_list = pd.read_csv('/workspaces/test/ycu-api/DataSet/english_words_list.csv',usecols=['Lemma\\n（見出し語）','Rank\\n(順位)','Frequency\\n(頻度)','Commentary\\n(解説)']).sort_values('Frequency\\n(頻度)',ascending=False)\n",
    "# en_word_list.dropna().info()\n",
    "print(en_word_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "\n",
    "en_word_list = pd.read_csv('/workspaces/test/ycu-api/DataSet/english_words_list.csv',usecols=['Lemma\\n（見出し語）','Rank\\n(順位)','Frequency\\n(頻度)','Commentary\\n(解説)']).sort_values('Frequency\\n(頻度)',ascending=False)\n",
    "\n",
    "\n",
    "def get_synonyms_w2v(text,model):\n",
    "    results = []\n",
    "    #for word, sim in model.wv.most_similar(text, topn=10):\n",
    "    for word, sim in model.most_similar(text, topn=20):\n",
    "        results.append({'term': word, 'similarity': sim})\n",
    "    return results\n",
    "\n",
    "def get_thema_w2v(text,model):\n",
    "    results = []\n",
    "    return_list = []\n",
    "    #for word, sim in model.wv.most_similar(text, topn=10):\n",
    "    thema_len = len(text)\n",
    "\n",
    "    for word, sim in model.most_similar(text, topn=100):        \n",
    "        word_len = len(word)\n",
    "        if thema_len > word_len:\n",
    "             r = max([SequenceMatcher(None, word, text[i:i+word_len]).ratio() for i in range(thema_len-word_len+1)])\n",
    "             if r >0.8:\n",
    "                 print(r,word)\n",
    "                 pass\n",
    "             else:\n",
    "                 results.append({'term': word, 'similarity': sim,'r':r})\n",
    "                 return_list.append(word)\n",
    "        elif thema_len > word_len:\n",
    "             r = max([SequenceMatcher(None, text, word[i:i+thema_len]).ratio() for i in range(word_len-thema_len+1)])\n",
    "             if r >0.8:\n",
    "                 print(r,word)\n",
    "                 pass\n",
    "             else:\n",
    "                 results.append({'term': word, 'similarity': sim,'r':r})\n",
    "                 return_list.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def calc_similarity_w2v(text1, text2,model):\n",
    "    sim = model.wv.similarity(text1, text2)\n",
    "    return sim\n",
    "\n",
    "def suggestion_thema(sentence_list,thema_list):\n",
    "    suggestion_thema_list = list(set(thema_list) - set(sentence_list))\n",
    "    print(len(suggestion_thema_list))\n",
    "    return en_word_list[en_word_list['Lemma\\n（見出し語）'].isin(suggestion_thema_list)].drop('Rank\\n(順位)', axis=1)\n",
    "\n",
    "# 英文の分解\n",
    "def sentence_to_word(text):\n",
    "    word_list = []\n",
    "    tmp_lst = text.replace('?','').replace('!','').replace('.','').split()                     \n",
    "    for word in tmp_lst:                       \n",
    "        word_list.append(word)  \n",
    "    return word_list\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I think that we should try to reduce trash and make the earth clean more. And usually we don't think of trash. But it is big prpblem to burn trash. Because when we burn it ,it relese CO2. So the earth is warmer and warmer. So we have to do something to reduce trash. We usually buy foods、cook and eat them. But we often throw away them that isn't eaten by us. I think that we stop this behavior. We should think amount of we can eat and buy foods So the earth will become clean and beautiful more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 enviroment\n",
      "0.9 evironment\n",
      "0.875 environs\n",
      "0.9 envionment\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "thema_list = get_thema_w2v('environment',model0)\n",
    "sentence_list = sentence_to_word(sentence)\n",
    "thema = suggestion_thema(sentence_list,thema_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['life', 12842.0, '生活. 一生. 人生. 生命. 命'], ['world', 12684.0, '世界'], ['system', 11725.0, 'システム. 組織. 仕組み'], ['area', 8441.0, '地域. エリア. 面積'], ['business', 7708.0, 'ビジネス. 仕事'], ['health', 5923.0, '健康. 健康状態. 調子'], ['culture', 4656.0, '文化'], ['community', 4162.0, '共同体. 生活共同体. 群生. 共同社会. 一般社会. 団体. 公衆'], ['technology', 3884.0, 'テクノロジー. 科学技術. 工芸学'], ['industry', 3469.0, '産業. 取り引き. 工業. 生産'], ['situation', 3021.0, '状態. 場所. 位置. 立場. 情勢'], ['education', 2877.0, '教育. 素養. 教育学'], ['economy', 2797.0, '経済. 経済組織. 節約'], ['society', 2510.0, '社会'], ['nature', 2085.0, '自然. 性質'], ['facility', 1759.0, 'たやすさ. 容易さ. 設備. 施設. 腕前'], ['organism', 1339.0, '有機体. 生物. 人間. 個々の小さな動･植物. 有機的組織体(社会など)'], ['context', 1297.0, '前後関係. 状況. 文脈. 背景'], ['attitude', 1070.0, '態度. 心構え. 姿勢'], ['manner', 1064.0, '方法. 態度. 行儀. 風習. 様式. 礼儀作法'], ['landscape', 736.0, 'の風景を美化する. 庭師をする. 景色. 風景画'], ['atmosphere', 636.0, '雰囲気. 環境. 空気. 気圧. 媒体ガス. 触媒. 大気'], ['pollution', 581.0, '公害. 汚すこと. 汚染. 汚れ'], ['planet', 545.0, '惑星'], ['climate', 519.0, '風土. 気候. 地方. 地帯. 土地'], ['lifestyle', 471.0, '生活様式'], ['biosphere', 299.0, 'バイオスフェア. 生物圏. 生活圏'], ['workplace', 293.0, '職場'], ['habitat', 269.0, '生息地. 環境. 居住環境. 生育地. すみか. 居住地'], ['wildlife', 249.0, '野生生物'], ['ecosystem', 149.0, '生態系'], ['terrain', 141.0, '(自然地理･軍事上から見た)地域. 地勢. 地形'], ['ecology', 100.0, '生態学. エコロジー. 人間生態学. 社会生態学. 生態環境'], ['upbringing', 72.0, '養育, 育て方, 躾(しつけ) '], ['mindset', 64.0, '考え方 '], ['milieu', 61.0, '(仏語. 中間の意). 環境. 境遇'], ['worldview', 57.0, '世界観 '], ['ambience', 45.0, '環境. 雰囲気'], ['fauna', 41.0, '(地方または一時代の)動物群. 動物区系. 動物相'], ['issues', 29.0, '問題. 課題 '], ['ambiance', 10.0, '空間. 環境. 雰囲気'], ['resources', 8.0, '資源. リソース']]\n"
     ]
    }
   ],
   "source": [
    "print(thema.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He ~ \n",
      "['he', '.He', '7.He', 'She', 'aHe', 'etc.He', 'It', 'too.He', '-He', '6.He', 'His', '20.He', 'IHe', 'him', '2.He', '4.He', '9.He', 'it-he', '-he', 'think.He']\n",
      "\n",
      "are ~ \n",
      "['were', 'aren', 'arer', 'ARE', 'arent', 'aare', \"'re\", 'They', 'sre', 'they', 'are--they', 'arere', 'may', 'iare', 'areing', 'will', 'have', 'arehe', 'wre', 'should']\n",
      "\n",
      "moving ~ \n",
      "['move', 'Moving', 'relocating', 'moved', 'shifting', 'moves', 'moving.If', 'moving.I', 'moving-', 'moveing', 'movng', 'settling', 'moving.And', 're-locating', 'moving.We', 'moving.The', 'moving.', 'moving.It', 'MOVING', 'dragging']\n",
      "\n",
      "here ~ \n",
      "['here.So', 'Here', 'here.But', 'here.Now', 'here.I', 'here.Well', 'here.As', 'here-', 'here.OK', 'here.That', 'here--', 'here.And', 'here.It', 'here.In', 'here.', 'here.If', 'here.We', 'here.Anyway', 'here.While', 'here.There']\n",
      "\n",
      "I ~ \n",
      "['So', \"'m\", \"'d\", '.I', '-I', 'so', 'am', 'we', 'iI', 'i', 'OK--I', 'if', 'think', 'thI', 'do-I', 'Ok', 'tI', 'sI', 'wI', 'weI']\n",
      "\n",
      "am ~ \n",
      "[\"'m\", 'I', 'Im', 'I.m', 'm', 'amIm', '.Im', 'Am', 'Iam', 'pmIm', 'im', '.I', 'Iʻm', 'amAm', '.Am', 'Hi', 'pmim', 'HiIm', 'Hi-I', 'amnot']\n",
      "\n",
      "doing ~ \n",
      "['doign', 'doind', 'doing.So', 'going', 'doing.Now', 'doing-', 'donig', 'doing--', 'doing.But', 'done', 'doing.I', 'doing.And', 'doing.That', 'doing--and', 'doing.It', 'doiing', 'Doing', 'putting', 'doing.In', 'working']\n",
      "\n",
      "fine ~ \n",
      "['dandy', 'fine-', 'not-so-fine', 'fine.And', 'fine.It', 'fine.This', 'fine.The', 'fine.All', 'fine.But', 'fine.I', 'fine.So', 'fine.Not', 'perfectly', 'fine.That', 'o.k.', 'fine.', 'fine.However', 'okay', 'fine.Now', 'alright']\n",
      "\n",
      "How ~ \n",
      "['What', '.How', 'Why', 'Does', 'HOw', 'Can', 'Did', 'Where', '?', 'too.How', '-How', 'Do', 'well.How', 'all.How', '3How', '2.How', 'it.How', '4.How', 'No.How', 'how']\n",
      "\n",
      "is ~ \n",
      "['It', 'be', 'This', \"'s\", 'Is', 'isng', 'so', 'it', 'IS', 'isn', 'was', 'a', 'isw', '.It', 'therefore', 'which', 'isre', 'is-it', \"'m\", 'That']\n",
      "\n",
      "you ~ \n",
      "['You', 'youou', 'youo', 'want', 'know', 'if', 'youy', 'oyou', 'your', 'fyou', 'youe', 'you.If', 'oyu', 'If', 'tyou', 'thyou', 'myou', 'yoyou', 'yoiu', 'syou']\n",
      "\n",
      "How ~ \n",
      "['What', '.How', 'Why', 'Does', 'HOw', 'Can', 'Did', 'Where', '?', 'too.How', '-How', 'Do', 'well.How', 'all.How', '3How', '2.How', 'it.How', '4.How', 'No.How', 'how']\n",
      "\n",
      "is ~ \n",
      "['It', 'be', 'This', \"'s\", 'Is', 'isng', 'so', 'it', 'IS', 'isn', 'was', 'a', 'isw', '.It', 'therefore', 'which', 'isre', 'is-it', \"'m\", 'That']\n",
      "\n",
      "they ~ \n",
      "['They', 'ththey', 'them', 'theyt', 'thhey', 'tehy', 'that--they', 'this--they', 'htey', 'are--they', 'not.They', 'theye', 'do--they', 'do.They', '-they', 'sthey', 'are.They', 'thehey', 'did.They', 'it-they']\n",
      "\n",
      "Matt ~ \n",
      "['Josh', 'Matthew', 'Chris', 'Mike', 'Greg', 'Tim', 'Nick', 'Jeff', 'Dan', 'Rob', 'Zach', 'Jason', 'Dave', 'Brian', 'Evan', '.Matt', 'Kevin', 'Jake', 'Jamie', 'Alex']\n",
      "\n",
      "like ~ \n",
      "['lke', 'llike', 'think', 'lilke', 'just', 'liek', 'really', 'feel', 'something', 'want', 'might', 'know', 'lik', 'like.I', 'would', 'Iike', 'look', 'likeed', 'liike', 'Like']\n",
      "\n",
      "fish ~ \n",
      "['fishes', 'fish.The', 'fish.But', 'fish.It', 'fish-', 'fish.I', 'fish.This', 'trout', 'fish.So', 'catfish', 'salmon', 'fish.', 'fish.You', '-fish', 'fish.There', 'non-fish', 'fish.And', 'fish.A', 'carp', 'cat-fish']\n",
      "\n",
      "the ~ \n",
      "['ththe', 'thehe', 'wthe', 'thethe', 'whthe', 'thithe', 'trhe', 'The', 'this', 'hte', 'it.The', 'athe', 'whhe', 'thihe', 'thahe', 'entire', 'tohe', 'ithe', 'theh', 'same']\n",
      "\n",
      "collection ~ \n",
      "['collections', 'collecton', 'collection.The', 'colection', 'colleciton', 'collectio', 'Collection', 'colletion', 'collections.The', 'collection.This', 'collecion', 'collecti', 'collection.', 'collection.It', 'colelction', 'collection.In', 'mini-collection', 'ollection', 'collection.A', 'pre-collection']\n",
      "\n",
      "of ~ \n",
      "['ofn', 'ofg', 'ofd', 'ofnt', 'ofhe', 'ofs', 'ofng', 'ofe', 'ofnd', 'ofof', 'ofss', 'ofry', 'ofith', 'ofor', 'ofut', 'ofes', 'ofy', 'ofom', 'ofion', 'ofh']\n",
      "\n",
      "letters ~ \n",
      "['letter', 'lettters', 'letters.The', 'letters.I', 'letters.', 'leters', 'letters.It', 'letters-', 'letters.In', 'lettter', 'lettes', 'Letters', 'letter-like', 'letters.This', 'letters.If', 'letter-writers', 'missives', 'telegrams', 'handwritten', 'alphabet']\n",
      "\n",
      "was ~ \n",
      "['were', 'wasn', 'had', 'wass', 'came', 'seemed', 'wasw', 'went', 'initially', 'waqs', 'meant', 'wasnt', 'originally', 'felt', 'wqas', 'knew', 'did', 'apparently', 'became', 'wwas']\n",
      "\n",
      "original ~ \n",
      "['orignal', 'orginal', 'origianl', 'orignial', 'origional', 'origial', 'origninal', 'original.The', 'Original', 'originals', 'oringinal', 'non-original', 'original.This', 'origina', 'origianal', 'originial', 'oringal', 'origonal', 'orginial', 'riginal']\n",
      "\n",
      "used ~ \n",
      "['utilized', 'use', 'utilised', 'used.I', 'commonly', 'employed', 'applied', 'uses', 'interchangeably', 'used.To', 'using', 'useed', 'synonymously', 'referred', 'used.', 're-used', 'intended', 'used.A', 'usede', 'Used']\n",
      "\n",
      "by ~ \n",
      "['byh', 'byhe', 'By', 'iby', 'byr', 'byat', 'byd', 'byof', 'byÂ', 'byy', 'byto', 'thby', 'tby', 'byan', 'byn', 'bya', 'fby', 'oby', 'byis', 'byf']\n",
      "\n",
      "the ~ \n",
      "['ththe', 'thehe', 'wthe', 'thethe', 'whthe', 'thithe', 'trhe', 'The', 'this', 'hte', 'it.The', 'athe', 'whhe', 'thihe', 'thahe', 'entire', 'tohe', 'ithe', 'theh', 'same']\n",
      "\n",
      "ancient ~ \n",
      "['Ancient', 'acient', 'antiquity', '2000-year-old', '3000-year-old', 'anicent', 'ncient', '3,000-year-old', 'prehistoric', 'pre-historic', '2500-year-old', 'ancients', 'medieval', '5,000-year-old', '1500-year-old', 'millennia-old', '2,000-year-old', '8,000-year-old', '2,500-year-old', 'theancient']\n",
      "\n",
      "Romans ~ \n",
      "['Galatians', 'Hebrews', 'romans', 'Ephesians', 'non-Romans', 'Carthaginians', 'Gauls', 'Romans.', 'Colossians', 'Etruscans', 'Greeks', 'Corinthians', '-Romans', 'Judeans', 'Thessalonians', 'Tacitus', 'Greco-Romans', 'Athenians', 'Thessalonian', 'Philippians']\n",
      "\n",
      "We ~ \n",
      "['we', 'So', 'I', '.We', 'If', '-We', 'aWe', 'oWe', 'so', 've', 'tWe', 'iWe', 'DoWe', '7We', 'll', 'us', 're', 'IWe', '.I', 'too.We']\n",
      "\n",
      "enjoys ~ \n",
      "['loves', 'adores', 'appreciates', 'relishes', 'Enjoys', 'enjoying', 'enjoyes', 'likes', 'cherishes', 'enjoy', 'spends', 'prefers', 'indulges', 'loathes', 'enoys', 'despises', 'ENJOYS', 'admires', 'tolerates', 'hates']\n",
      "\n",
      "horror ~ \n",
      "['horro', 'horror-', 'horror.The', 'non-horror', 'horror-film', 'horror.I', 'horrow', 'horror.', 'body-horror', 'horror-movie', 'horrors', 'horrorific', 'gorefests', 'gorefest', 'Horror', 'horror-ish', 'horror-comedy', 'gore-fest', 'B-horror', 'horror-based']\n",
      "\n",
      "movies ~ \n",
      "['films', 'movie', 'moives', 'movies.But', 'movies.So', 'movies.If', 'movies.I', 'movies.And', 'movies-', 'movies.It', 'movies.This', 'movies.What', 'movies.', 'movies.As', 'movies.In', 'movies.The', 'Movies', 'movies.There', 'movies--', 'movies.You']\n",
      "\n",
      "Anna ~ \n",
      "['Maria', 'Elena', 'Emma', 'Karina', 'Julia', 'Christina', 'Kristina', 'Elsa', 'Nicole', 'Elisabeth', 'Emily', 'Sophia', 'Caroline', 'Johanna', 'Katherine', 'Eva', 'Susanna', 'Rebecca', 'Laura', 'Natalia']\n",
      "\n",
      "and ~ \n",
      "['andnd', 'andn', 'andt', 'ande', 'andng', 'but', 'while', 'andnt', 'then', 'oand', 'both', 'andf', ',', 'with', 'They', 'which', 'including', 'whnd', 'andtion', 'adn']\n",
      "\n",
      "Mike ~ \n",
      "['Dave', 'Doug', 'Greg', 'Jim', 'Chris', 'Steve', 'Kevin', 'Brian', 'Jeff', 'MIke', 'Dan', 'Phil', 'Tim', 'Rob', 'Matt', 'Nick', 'Jason', 'Andy', 'Rick', 'Randy']\n",
      "\n",
      "is ~ \n",
      "['It', 'be', 'This', \"'s\", 'Is', 'isng', 'so', 'it', 'IS', 'isn', 'was', 'a', 'isw', '.It', 'therefore', 'which', 'isre', 'is-it', \"'m\", 'That']\n",
      "\n",
      "going ~ \n",
      "['gonna', 'goign', 'coming', 'doing', 'goint', 'supposed', 'getting', 'gonig', 'giong', 'trying', 'goiing', 'just', 'really', 'goind', 'want', 'gunna', 'goig', 'go', 'talking', 'not']\n",
      "\n",
      "skiing ~ \n",
      "['skiiing', 'ski', 'snowboarding', 'ski-ing', 'Skiing', 'skiing.', 'snow-boarding', 'skiing.The', 'skiers', 'snow-skiing', 'ski-', 'skier', 'snowshoeing', 'skiier', 'non-ski', 'non-skier', 'skiiers', 'skis', 'non-skiing', 'ski-touring']\n",
      "\n",
      "I ~ \n",
      "['So', \"'m\", \"'d\", '.I', '-I', 'so', 'am', 'we', 'iI', 'i', 'OK--I', 'if', 'think', 'thI', 'do-I', 'Ok', 'tI', 'sI', 'wI', 'weI']\n",
      "\n",
      "walk ~ \n",
      "['stroll', 'walking', 'walks', 'wallk', 'walk.I', 'walk.', 'walk.If', 'walked', 'walk.The', 'walk.And', 'power-walk', 'wlak', '-walk', 'walk.There', 'walk.What', 'walk.So', 'walk.It', 'saunter', 'walk.This', 'walk.But']\n",
      "\n",
      "to ~ \n",
      "['go', 'To', 'to.To', 'do', 'able', 'toce', 'yto', 'whto', 'frto', 'tohe', 'Tto', 'toou', 'bto', 'thto', 'toow', 'tond', 'try', 'be', 'prto', 'thito']\n",
      "\n",
      "the ~ \n",
      "['ththe', 'thehe', 'wthe', 'thethe', 'whthe', 'thithe', 'trhe', 'The', 'this', 'hte', 'it.The', 'athe', 'whhe', 'thihe', 'thahe', 'entire', 'tohe', 'ithe', 'theh', 'same']\n",
      "\n",
      "store ~ \n",
      "['stores', 'store.The', 'store.But', 'store.And', 'shop', 'store.So', 'store.Now', 'store--', 'store.As', 'store.This', 'store.I', 'store.In', 'store.It', 'store.That', 'store.They', 'store.', 'store.Then', 'super-store', 'store.For', 'store-like']\n",
      "\n",
      "and ~ \n",
      "['andnd', 'andn', 'andt', 'ande', 'andng', 'but', 'while', 'andnt', 'then', 'oand', 'both', 'andf', ',', 'with', 'They', 'which', 'including', 'whnd', 'andtion', 'adn']\n",
      "\n",
      "I ~ \n",
      "['So', \"'m\", \"'d\", '.I', '-I', 'so', 'am', 'we', 'iI', 'i', 'OK--I', 'if', 'think', 'thI', 'do-I', 'Ok', 'tI', 'sI', 'wI', 'weI']\n",
      "\n",
      "bought ~ \n",
      "['purchased', 'puchased', 'purchsed', 'purcahsed', 'pruchased', 'Bought', 'sold', 'purhased', 'bougt', 're-bought', 'baught', 'bougth', 'splurged', 'buy', 'purchaced', 'purhcased', 'Ibought', 're-purchased', 'rebought', 'bought.I']\n",
      "\n",
      "milk ~ \n",
      "['milk.The', 'milk.A', 'milk.', 'milk-', 'milk.If', 'milk.This', 'milk.It', 'milk.I', 'milk.And', 'milk.You', 'milk.We', 'milk.In', 'millk', 'non-milk', 'milks', 'breastmilk', 'dairy', 'breast-milk', 'full-cream', 'colostrum']\n",
      "\n",
      "We ~ \n",
      "['we', 'So', 'I', '.We', 'If', '-We', 'aWe', 'oWe', 'so', 've', 'tWe', 'iWe', 'DoWe', '7We', 'll', 'us', 're', 'IWe', '.I', 'too.We']\n",
      "\n",
      "all ~ \n",
      "['those', 'these', 'alll', 'sorts', 'ALL', 'everything', 'some', 'All', 'kinds', 'many', 'both', 'other', 'everyone', 'them.All', 'none', 'it.All', 'them', 'things', 'thoses', 'the']\n",
      "\n",
      "eat ~ \n",
      "['ate', 'eating', 'eat.But', 'eats', 'eat.So', 'eaten', 'eat.If', 'eat.What', 'eat.When', 'eat.And', 'eat.', 'over-eat', 'eat.We', 'eat.I', 'eat.This', 'eat.It', 'eat.You', 'Eat', 'eat--', 'eat.In']\n",
      "\n",
      "the ~ \n",
      "['ththe', 'thehe', 'wthe', 'thethe', 'whthe', 'thithe', 'trhe', 'The', 'this', 'hte', 'it.The', 'athe', 'whhe', 'thihe', 'thahe', 'entire', 'tohe', 'ithe', 'theh', 'same']\n",
      "\n",
      "fish ~ \n",
      "['fishes', 'fish.The', 'fish.But', 'fish.It', 'fish-', 'fish.I', 'fish.This', 'trout', 'fish.So', 'catfish', 'salmon', 'fish.', 'fish.You', '-fish', 'fish.There', 'non-fish', 'fish.And', 'fish.A', 'carp', 'cat-fish']\n",
      "\n",
      "and ~ \n",
      "['andnd', 'andn', 'andt', 'ande', 'andng', 'but', 'while', 'andnt', 'then', 'oand', 'both', 'andf', ',', 'with', 'They', 'which', 'including', 'whnd', 'andtion', 'adn']\n",
      "\n",
      "then ~ \n",
      "['Then', 'THEN', 'eventually', 'once', 'when', 'THen', 'until', 'again', 'just', 'before', 'lastly', '.Then', 'first.Then', 'Once', 'Afterwards', 'whereupon', 'too.Then', 'did.Then', 'afterwards', 'where']\n",
      "\n",
      "made ~ \n",
      "['make', 'making', 'made.So', 'mades', 'made.When', 'made.I', 'made.It', 'made.The', 'made.But', 'made.A', 'made.', 'made.This', 'made.One', 'Made', 'made.And', 'made.He', 'made.That', 'made.Now', 'made.What', 'made.For']\n",
      "\n",
      "dessert ~ \n",
      "['desserts', 'dessert-', 'Dessert', 'dessert.This', 'dessert.The', 'tiramisu', 'cheesecake', 'dessert.I', 'dessert.', 'non-dessert', 'appetizer', 'pre-dessert', 'Tiramisu', 'desser', 'dessert.We', 'dessert-style', 'desserts-', 'sorbet', 'semifreddo', 'dessert-y']\n",
      "\n",
      "I ~ \n",
      "['So', \"'m\", \"'d\", '.I', '-I', 'so', 'am', 'we', 'iI', 'i', 'OK--I', 'if', 'think', 'thI', 'do-I', 'Ok', 'tI', 'sI', 'wI', 'weI']\n",
      "\n",
      "will ~ \n",
      "['willl', 'wil', 'should', 'wiill', 'can', 'may', 'wll', \"'ll\", 'would', 'wiil', 'might', 'must', 'WILL', 'continue', 'awill', 'hopefully', 'be', 'surely', 'wwill', 'owill']\n",
      "\n",
      "eat ~ \n",
      "['ate', 'eating', 'eat.But', 'eats', 'eat.So', 'eaten', 'eat.If', 'eat.What', 'eat.When', 'eat.And', 'eat.', 'over-eat', 'eat.We', 'eat.I', 'eat.This', 'eat.It', 'eat.You', 'Eat', 'eat--', 'eat.In']\n",
      "\n",
      "fish ~ \n",
      "['fishes', 'fish.The', 'fish.But', 'fish.It', 'fish-', 'fish.I', 'fish.This', 'trout', 'fish.So', 'catfish', 'salmon', 'fish.', 'fish.You', '-fish', 'fish.There', 'non-fish', 'fish.And', 'fish.A', 'carp', 'cat-fish']\n",
      "\n",
      "for ~ \n",
      "['fofor', 'foror', 'tfor', 'For', 'forng', 'thfor', 'fot', 'ofor', 'foring', 'wfor', 'forfor', 'ofr', 'fory', 'fornd', 'afor', 'mfor', 'sfor', 'fopr', 'for.For', 'fothe']\n",
      "\n",
      "dinner ~ \n",
      "['supper', 'dinners', 'meal', 'lunch', 'dinner-', 'dinne', 'dinner.But', 'Dinner', 'dinner.This', 'dinner.The', 'dinner.That', 'dinnner', 'dinner.It', 'dinner.We', 'dinner.So', 'dinner.I', 'dinner--', 'dinner.And', 'dinner.Now', 'dinner.As']\n",
      "\n",
      "and ~ \n",
      "['andnd', 'andn', 'andt', 'ande', 'andng', 'but', 'while', 'andnt', 'then', 'oand', 'both', 'andf', ',', 'with', 'They', 'which', 'including', 'whnd', 'andtion', 'adn']\n",
      "\n",
      "drink ~ \n",
      "['drinks', 'drinking', 'drink.I', 'drank', 'drink.', 'drink.And', 'drink.This', 'drink.You', 'beverage', 'drink.The', 'drink.A', 'drink.So', 'Drink', 'drink-', 'drink.It', 'sip', 'drink.There', 'drink.In', 'beverages', 'drink.If']\n",
      "\n",
      "milk ~ \n",
      "['milk.The', 'milk.A', 'milk.', 'milk-', 'milk.If', 'milk.This', 'milk.It', 'milk.I', 'milk.And', 'milk.You', 'milk.We', 'milk.In', 'millk', 'non-milk', 'milks', 'breastmilk', 'dairy', 'breast-milk', 'full-cream', 'colostrum']\n",
      "\n",
      "what ~ \n",
      "['exactly', 'how', 'waht', 'know', 'why', 'that.What', 'that', 'think', 'it.What', 'whaty', 'What', 'whatever', 'is.What', 'whats', 'exaclty', 'EXACTLY', 'do.What', 'whether', 'whay', 'thing']\n",
      "\n",
      "be ~ \n",
      "['is', 'will', 'not', 'should', 'able', 'thbe', 'so', 'may', 'can', 'might', 'bе', 'considered', 'would', 'could', \"'t\", 'if', 'go', 'to', 'going', 'must']\n",
      "\n",
      "the ~ \n",
      "['ththe', 'thehe', 'wthe', 'thethe', 'whthe', 'thithe', 'trhe', 'The', 'this', 'hte', 'it.The', 'athe', 'whhe', 'thihe', 'thahe', 'entire', 'tohe', 'ithe', 'theh', 'same']\n",
      "\n",
      "reason ~ \n",
      "['reasons', 'why', 'reaon', 'reason.So', 'reason.The', 'reson', 'reaons', 'reason.And', 'because', 'reason.Now', 'reason.I', 'reason.That', 'reason--the', 'reason.But', 'thing', 'reason.', 'reason.If', 'reason.It', 'reason.There', 'reasone']\n",
      "\n",
      "for ~ \n",
      "['fofor', 'foror', 'tfor', 'For', 'forng', 'thfor', 'fot', 'ofor', 'foring', 'wfor', 'forfor', 'ofr', 'fory', 'fornd', 'afor', 'mfor', 'sfor', 'fopr', 'for.For', 'fothe']\n",
      "\n",
      "everyone ~ \n",
      "['everybody', 'eveyone', 'EVERYONE', 'Everyone', 'everone', 'everbody', 'EVERYBODY', 'eveybody', 'every-one', 'nobody', 'EVERYone', 'everyon', 'anyone', 'evryone', 'Everybody', 'eveyrone', 'else', 'anybody', 'Eveyone', 'it.Everyone']\n",
      "\n",
      "leave ~ \n",
      "['leaving', 'leave.If', 'leave.I', 'left', 'let', 'leave.But', 'leaves', 'leave.You', 'leave.And', 'leave.They', 'Leaving', 'leave.The', 'behind.If', 'leavea', 'leave.It', 'leave.', 'leave.We', 'leave.As', 'rejoin', 'abandon']\n",
      "\n",
      "the ~ \n",
      "['ththe', 'thehe', 'wthe', 'thethe', 'whthe', 'thithe', 'trhe', 'The', 'this', 'hte', 'it.The', 'athe', 'whhe', 'thihe', 'thahe', 'entire', 'tohe', 'ithe', 'theh', 'same']\n",
      "\n",
      "company ~ \n",
      "['compnay', 'comapny', 'comany', 'company.It', 'companies', 'company.The', 'companys', 'compamy', 'corporation', 'company.Now', 'comapany', 'company.They', 'compay', 'company.This', 'Company', 'company.That', 'sub-company', 'company.In', 'company.A', 'company.When']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probe_words =  sentence_to_word(sentence)\n",
    "\n",
    "for probe_word in probe_words:\n",
    "    word_similar=get_synonyms_w2v(probe_word,model0)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{probe_word} ~ \\n{words}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 〜　\n",
      "['cats', 'kitty', 'kitten', 'feline', 'moggie', 'cat.It', 'dog', 'cat.The', 'cat.I', 'moggy']\n",
      "\n",
      "dog 〜　\n",
      "['dogs', 'puppy', 'pup', 'canine', 'pet', 'doggie', 'dog--', 'beagle', 'dachshund', 'cat']\n",
      "\n",
      "NewYork 〜　\n",
      "['York', 'Yorkand', 'New-York', 'Newyork', 'York.', 'YorkCity', 'YOrk', 'York.I', 'York-', 'N.Y']\n",
      "\n",
      "children 〜　\n",
      "['kids', 'chidren', 'chidlren', 'chldren', 'parents', 'adults', 'chilren', 'childred', 'child', 'chiildren']\n",
      "\n",
      "school 〜　\n",
      "['schoo', 'schools', 'school.The', 'schoool', 'kindergarten', 'shcool', 'school.All', 'shool', 'school.And', 'school-']\n",
      "\n",
      "govement 〜　\n",
      "['governemt', 'govenment', 'goverenment', 'govenrment', 'governmetn', 'governement', 'governmet', 'goverment', 'govrnment', 'gvt']\n",
      "\n",
      "comment 〜　\n",
      "['comments', 'commment', 'commenting', 'comment.', 'comment.This', 'commments', 'comment.And', 're-comment', 'comment.That', 'post']\n",
      "\n",
      "summer 〜　\n",
      "['summertime', 'winter', 'summers', 'mid-summer', 'summmer', 'autumn', 'spring', 'summer.This', 'Summer', 'summer-']\n",
      "\n",
      "WTO 〜　\n",
      "['W.T.O.', 'GATT', 'USTR', 'TPP', 'TRIPs', 'FTAA', 'plurilateral', 'FTAs', 'RCEP', 'APEC']\n",
      "\n",
      "SARS 〜　\n",
      "['Ebola', 'H1N1', 'pandemic', 'bird-flu', 'H7N9', 'H5N1', 'EBOLA', 'Sars', 'SARS-like', 'ebola']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# probe_words=['cat','dog','NewYork',\n",
    "#              #'自由の女神',\n",
    "#              'children',\n",
    "#              'school',\n",
    "#              'govement',\n",
    "#              #'6月',\n",
    "#              #'テレビドラマ',\n",
    "#              'comment',\n",
    "#              'summer',\n",
    "#              'WTO',\n",
    "#              'SARS',\n",
    "#             ]\n",
    "\n",
    "# for probe_word in probe_words:\n",
    "#     word_similar=get_synonyms_w2v(probe_word,model0)\n",
    "#     words=[word_entry['term'] for word_entry in word_similar]\n",
    "#     print(f'{probe_word} 〜　\\n{words}\\n')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アナロジー　（意味の加減算)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(X_Y, x,model):\n",
    "    X, Y = X_Y\n",
    "    results = []\n",
    "    #for word, sim in model.wv.most_similar(positive=[Y, x], negative=[X], topn=10):\n",
    "    for word, sim in model.most_similar(positive=[Y, x], negative=[X], topn=10):\n",
    "        results.append({'term': word, 'similarity': sim})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris - Franch + Germany 〜　\n",
      "['Berlin', 'Munich', 'Hamburg', 'Frankfurt', 'Leipzig', 'Düsseldorf', 'Cologne', 'Stuttgart', 'Vienna', 'Bonn']\n",
      "\n",
      "London - England + Japan 〜　\n",
      "['Tokyo', 'Toyko', 'Osaka', 'Tokyo-', 'Fukuoka', 'Shinjuku', 'Nagoya', 'Tokyo.The', 'Yokohama', 'Shibuya']\n",
      "\n",
      "girl - boy + husband 〜　\n",
      "['wife', 'fiance', 'mother-in-law', 'sister-in-law', 'daughter', 'ex-husband', 'huband', 'step-daughter', 'fiancé', 'fiancee']\n",
      "\n",
      "Tokyo - Japan + Korea 〜　\n",
      "['Seoul', 'Pyongyang', 'Daegu', 'Gwangju', 'Busan', 'Seoul.The', 'Pyonyang', 'Daejeon', 'Kwangju', 'Gyeongsan']\n",
      "\n",
      "Tokyo - Japan + China 〜　\n",
      "['Beijing', 'Shanghai', 'Chongqing', 'Bejing', 'Nanjing', 'Shenzhen', 'Chengdu', 'Hangzhou', 'Guangzhou', 'Tianjin']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_Y_x_list=[( 'Franch','Paris','Germany'),  #首都\n",
    "            ('England','London','Japan'),  #首都\n",
    "            ('boy','girl','husband'),  #性別\n",
    "            ('Japan','Tokyo', 'Korea' ),  # NG 韓国がボキャブラリー辞書にない\n",
    "            ('Japan','Tokyo', 'China'),   # NG　中国がボキャブラリー辞書にない\n",
    "           ]\n",
    "\n",
    "for X,Y, x in X_Y_x_list:\n",
    "    word_similar=analogy((X,Y),x,model0)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{Y} - {X} + {x} 〜　\\n{words}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日本語Wikipedia 全ページの文章でWord2Vecを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "#import lec05_parser as parser\n",
    "import MeCab\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クレンジング：正規化と空白の圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_table=str.maketrans(dict(zip('()!', '（）！')))  # 半角 -> 全角  \n",
    "#\n",
    "#  クレンジング：　正規化と，　空白の圧縮\n",
    "#\n",
    "def cleanse(text,debug=False):\n",
    "    if debug:\n",
    "        return text\n",
    "    text=unicodedata.normalize('NFKC', text).translate(translation_table)  #  テキストの正規化\n",
    "    text=re.sub(r'\\s+', ' ',text)  # 空白の正規化 半角の空白1つに。\n",
    "    #text=cleanse_post(text)\n",
    "    return text\n",
    "\n",
    "def insert_nl_at_eos_char(text):  # insert newline's\n",
    "    text=re.sub('([。])', '\\\\1\\n',text)\n",
    "    return text\n",
    "\n",
    "def cleanse_ext(text):  #  for word2vec special\n",
    "    text=re.sub('Section::::','',text)\n",
    "    text=re.sub('[！「」『』（），、]','',text)\n",
    "    return text\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文単位で分かち書きされたファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパス作り\n",
    "WIKIDUMP_DIR='./wikipedia_text'\n",
    "eos_mark='<EOS>'\n",
    "\n",
    "def get_each_contetns_by_text(wikidir):\n",
    "    for file in tqdm(glob(os.path.join(wikidir,'*/wiki_*'))):\n",
    "        #print(file)\n",
    "        with open(file,'r') as f:\n",
    "            read_text=f.read()\n",
    "            soup=BeautifulSoup(read_text)\n",
    "            for doc in soup.find_all(['doc']):\n",
    "                text=''\n",
    "                input_text=doc.text\n",
    "                input_text=cleanse(input_text)\n",
    "                input_text=insert_nl_at_eos_char(input_text)\n",
    "                for line in input_text.split('\\n'):\n",
    "                    line=line.strip()\n",
    "                    if len(line)>0:\n",
    "                        #print(f'**{line}**')\n",
    "                        text += cleanse_ext(line)\n",
    "                        if line[-1] in ['。','！']:\n",
    "                            text=text[:-1]+eos_mark  # remove ['。','！'] but insert eos\n",
    "                        else:\n",
    "                            text+=eos_mark\n",
    "                            #\n",
    "                            pass\n",
    "                        text += '\\n'\n",
    "                title=doc['title']\n",
    "                yield cleanse(text), cleanse(title)\n",
    "                #yield text, title\n",
    "    \n",
    "def convert_and_concat_wakati(out_dir='.'):\n",
    "    with open(os.path.join(out_dir,'wiki_wakati_nl_Q.txt'),'w') as fw,\\\n",
    "        open(os.path.join(out_dir,'wiki_title_Q.txt'),'w') as f_title:\n",
    "        count=0    \n",
    "        #mecab=parser.MeCab('-Owakati')\n",
    "        mecab= MeCab.Tagger('-Owakati')\n",
    "        for text, title in  get_each_contetns_by_text(WIKIDUMP_DIR):\n",
    "            print('　'*60,end='\\r')\n",
    "            print(f'{count}:{title[:45]}',end='\\r')\n",
    "            f_title.write(f'{count}:{title}\\n')\n",
    "            #print(mecab.parse(text))\n",
    "            wakati_words=mecab.parse(text)\n",
    "            #wakati_words=wakati.parse(text)   # こっちは エッフェル塔 が分解される\n",
    "            wakati_words=repair_eos(wakati_words)\n",
    "            fw.write(wakati_words)\n",
    "            #print(text)\n",
    "            if count>1:\n",
    "                #break\n",
    "                pass\n",
    "            count += 1\n",
    "            #if count>5: break  # for debug\n",
    "        print('done')\n",
    "        del mecab\n",
    "        \n",
    "def repair_eos(text):\n",
    "    text=re.sub('< EOS > ','\\n', text)\n",
    "    return text\n",
    "    \n",
    "# def repair_eos(text):\n",
    "#     text = '<SOS> '+text\n",
    "#     text=re.sub('< EOS >','<EOS>\\n<SOS>', text)\n",
    "#     text=re.sub('<SOS>$','',text)\n",
    "#     #print(f'* {text} *')\n",
    "#     return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分かち書き処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "convert_and_concat_wakati(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  実行の記録\n",
    "2020/11/11 <br>\n",
    "```\n",
    "見出し数：1,179,168\n",
    "分かち書き処理：\n",
    "CPU times: user 20min 54s, sys: 3min 18s, total: 24min 12s\n",
    "Wall time: 1h 18min 2s\n",
    "```\n",
    "\n",
    "2021/11/18<br>\n",
    "```\n",
    "done101:福生市立福生第五小学校　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\n",
    "CPU times: user 21min 48s, sys: 1min 49s, total: 23min 38s\n",
    "Wall time: 24min 26s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensimのword2vec で\n",
    "# 日本語wikipedia全コンテンツを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "vector_size=100\n",
    "vector_size=300\n",
    "%time data = word2vec.Text8Corpus(os.path.join(out_dir,'wiki_wakati_nl_Q.txt'))\n",
    "%time model =  word2vec.Word2Vec(data, vector_size=vector_size)  # 100次元で学習\n",
    "#\n",
    "out_fname=f\"wiki_{vector_size}.model\"\n",
    "out_path=os.path.join(out_dir,out_fname)\n",
    "print(f'saving {out_path} ... ')\n",
    "%time model1.save(out_path)  \n",
    "print(\"ok\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_fname=f\"wiki_{vector_size}.model\"\n",
    "out_path=os.path.join(out_dir,out_fname)\n",
    "print(f'saving {out_path} ... ')\n",
    "%time model1.save(out_path)  \n",
    "print(\"ok\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  実行の記録\n",
    "2020/11/09 <br>\n",
    "```\n",
    "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
    "Wall time: 9.06 µs\n",
    "CPU times: user 1h 34min 30s, sys: 1min 7s, total: 1h 35min 38s\n",
    "Wall time: 36min 14s\n",
    "CPU times: user 2.07 s, sys: 1.13 s, total: 3.2 s\n",
    "Wall time: 3.9 s\n",
    "ok\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "ファイルサイズ：\n",
    "wiki_wakati_nl.txt                 3,147,077,302 バイト（ディスク上の3.15 GB）\n",
    "wiki.model                         67,779,585 バイト（ディスク上の70.8 MB）\n",
    "wiki.model.trainables.syn1neg.npy  373,465,328 バイト（ディスク上の376.9 MB）\n",
    "wiki.model.wv.vectors.npy          373,465,328 バイト（ディスク上の375.7 MB）\n",
    "```\n",
    "\n",
    "2021.11.18<br>\n",
    "```\n",
    "vector_size=100\n",
    "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
    "Wall time: 11.9 µs\n",
    "CPU times: user 1h 17min 40s, sys: 41.5 s, total: 1h 18min 21s\n",
    "Wall time: 27min 34s\n",
    "CPU times: user 210 ms, sys: 920 ms, total: 1.13 s\n",
    "Wall time: 1.19 s\n",
    "ok\n",
    "```\n",
    "\n",
    "生成ファイル<br>\n",
    "```\n",
    "15787032  11 18 14:19 wiki.model\n",
    "177484928 11 18 14:19 wiki.model.syn1neg.npy\n",
    "177484928 11 18 14:19 wiki.model.wv.vectors.npy\n",
    "```\n",
    "\n",
    "```\n",
    "vector_size=300\n",
    "CPU times: user 8 µs, sys: 20 µs, total: 28 µs\n",
    "Wall time: 31 µs\n",
    "CPU times: user 1h 48minCPU times: user 8 µs, sys: 20 µs, total: 28 µs\n",
    "Wall time: 31 µs\n",
    "CPU times: user 1h 48min\n",
    "saving wiki_300.model ... \n",
    "CPU times: user 249 ms, sys: 2.2 s, total: 2.45 s\n",
    "Wall time: 2.67 s\n",
    "oksaving wiki_300.model ... \n",
    "CPU times: user 249 ms, sys: 2.2 s, total: 2.45 s\n",
    "Wall time: 2.67 s\n",
    "ok\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習した Word2Vec の利用 (モデルロード）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "vector_size=100\n",
    "vector_size=300\n",
    "\n",
    "in_fname=f\"wiki_{vector_size}.model\"\n",
    "in_path=os.path.join(out_dir,in_fname)\n",
    "print(f'loading {in_path} ... ')\n",
    "model1 = gensim.models.Word2Vec.load(in_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同義語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_words=['フランス','東京','ニューヨーク',\n",
    "             #'自由の女神','ウルトラマン',\n",
    "             #'中学校',\n",
    "             '政府',\n",
    "             #'6月',\n",
    "             #'テレビドラマ',\n",
    "             'ピカソ','夏','WTO','SARS','コロナ']\n",
    "for probe_word in probe_words:\n",
    "    word_similar=get_synonyms_w2v(probe_word,model1.wv)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{probe_word} 〜　\\n{words}\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# アナロジー　（意味の加減算)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size=100\n",
    "in_fname=f\"wiki_{vector_size}.model\"\n",
    "in_path=os.path.join(out_dir,in_fname)\n",
    "print(f'loading {in_path} ... ')\n",
    "model100 = gensim.models.Word2Vec.load(in_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Y_x_list=[( 'フランス','パリ','ドイツ'),  #首都\n",
    "            ('イギリス','ロンドン','日本'),  #首都\n",
    "            ('日本','東京','ドイツ'),  #首都\n",
    "            ('少年','少女','夫'),  #性別\n",
    "            ('日本','東京', '韓国' ),  # NG 韓国がボキャブラリー辞書にない\n",
    "            ('日本','東京', '中国'),   # NG　中国がボキャブラリー辞書にない\n",
    "            #('パリ', 'エッフェル塔','東京'), # 観光資源\n",
    "           ]\n",
    "\n",
    "for X,Y, x in X_Y_x_list:\n",
    "    print('model_300')\n",
    "    print('*'*30)\n",
    "    word_similar=analogy((X,Y),x,model1.wv)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{Y} - {X} + {x} 〜　\\n{words}\\n')\n",
    "    #----\n",
    "    print('model_100')\n",
    "    word_similar=analogy((X,Y),x,model100.wv)\n",
    "    words=[word_entry['term'] for word_entry in word_similar]\n",
    "    print(f'{Y} - {X} + {x} 〜　\\n{words}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ <br>\n",
    "\n",
    "# ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ <br>\n",
    "\n",
    "\n",
    "# ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ ★ <br>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
